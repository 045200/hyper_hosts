import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
import chardet

# é…ç½®å‚æ•°
MAX_RETRIES = 3
REQUEST_TIMEOUT = 20
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, text like Gecko) Chrome/122.0.0.0 Safari/537.36 GitHubHostsMerger/1.0"

# å¸¦é‡è¯•æœºåˆ¶çš„Sessioné…ç½®
def create_session():
    session = requests.Session()
    retries = Retry(
        total=MAX_RETRIES,
        backoff_factor=0.5,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=frozenset(['GET'])
    )
    session.mount('https://', HTTPAdapter(max_retries=retries))
    session.mount('http://', HTTPAdapter(max_retries=retries))
    return session

def detect_encoding(content):
    # ä½¿ç”¨åŒé‡æ£€æµ‹æœºåˆ¶æé«˜å‡†ç¡®ç‡
    try:
        result = chardet.detect(content)
        if result['confidence'] > 0.7:
            return result['encoding']
        return 'utf-8'
    except Exception:
        return 'utf-8'

def fetch_and_merge_hosts():
    urls = [
        "https://raw.githubusercontent.com/jdlingyu/ad-wars/master/hosts",
        "https://lingeringsound.github.io/10007_auto/reward",
        "https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/Filters/AWAvenue-Ads-Rule-hosts.txt",
"https://raw.githubusercontent.com/ineo6/hosts/refs/heads/master/hosts"
    ]

    unique_entries = []
    seen = set()
    session = create_session()
    
    start_time = time.time()
    success_count = 0

    for idx, url in enumerate(urls, 1):
        try:
            print(f"ğŸ” æ­£åœ¨å¤„ç†æº({idx}/{len(urls)})ï¼š{url}")
            response = session.get(
                url,
                timeout=REQUEST_TIMEOUT,
                headers={'User-Agent': USER_AGENT},
                verify=True  # ä¿æŒSSLéªŒè¯
            )
            response.raise_for_status()
            
            # æ™ºèƒ½ç¼–ç æ£€æµ‹
            detected_encoding = detect_encoding(response.content)
            try:
                content = response.content.decode(detected_encoding)
            except UnicodeDecodeError:
                content = response.content.decode('utf-8', errors='replace')

            line_count = 0
            for raw_line in content.splitlines():
                line = raw_line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # è§„èŒƒåŒ–æ¡ç›®ï¼ˆå¯é€‰æ‰©å±•ç‚¹ï¼‰
                clean_line = line.split('#')[0].strip()  # å»é™¤è¡Œå†…æ³¨é‡Š
                if clean_line and clean_line not in seen:
                    seen.add(clean_line)
                    unique_entries.append(clean_line)
                    line_count += 1

            success_count += 1
            print(f"âœ… æˆåŠŸè·å–ï¼š{url} | æ–°å¢æ¡ç›®ï¼š{line_count} | å½“å‰æ€»æ•°ï¼š{len(unique_entries)}")
            
        except Exception as e:
            err_msg = f"âŒ æºå¤„ç†å¤±è´¥ï¼š{url} | é”™è¯¯ç±»å‹ï¼š{type(e).__name__}"
            if hasattr(e, 'response'):
                err_msg += f" | çŠ¶æ€ç ï¼š{e.response.status_code}"
            print(err_msg)
            continue

    # å†™å…¥ä¼˜åŒ–
    if unique_entries:
        with open("hosts.txt", "w", encoding="utf-8", newline='\n') as f:
            f.write(f"# Generated by GitHub Actions at {time.strftime('%Y-%m-%d %H:%M:%S UTC%z')}\n")
            f.write("\n".join(unique_entries))
        
        elapsed = time.time() - start_time
        stats = f"""
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         åˆå¹¶å®ŒæˆæŠ¥å‘Š          â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘ æˆåŠŸæº        â•‘ {success_count}/{len(urls)}       â•‘
        â•‘ æ€»æ¡ç›®æ•°      â•‘ {len(unique_entries):<13} â•‘
        â•‘ è€—æ—¶          â•‘ {elapsed:.2f}s       â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(stats)
    else:
        print("âš ï¸ è­¦å‘Šï¼šæœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ¡ç›®")

if __name__ == "__main__":
    fetch_and_merge_hosts()
