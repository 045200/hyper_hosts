import os
import requests
import re
from datetime import datetime

# åŸºç¡€è·¯å¾„é…ç½®ï¼ˆå…¨éƒ¨ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
TEMP_DIR = os.path.join(DATA_DIR, "python")
RULE_DIR = os.path.join(DATA_DIR, "rules")

# ä¸‹è½½æºé…ç½®
HOSTS_SOURCES = [
    "https://raw.githubusercontent.com/jdlingyu/ad-wars/master/hosts",
    "https://raw.githubusercontent.com/lingeringsound/10007_auto/master/reward",
    "https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/Filters/AWAvenue-Ads-Rule-hosts.txt",
    "https://raw.githubusercontent.com/ineo6/hosts/refs/heads/master/hosts"
]

def ensure_directory(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    dir_path = os.path.dirname(path)
    os.makedirs(dir_path, exist_ok=True)
    print(f"[ç›®å½•å·²åˆ›å»º] {dir_path}")

def download_hosts():
    """ä¸‹è½½å¤šæºhostsæ–‡ä»¶"""
    downloaded_files = []
    for index, url in enumerate(HOSTS_SOURCES):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            filename = f"hosts_{index}.txt"
            save_path = os.path.join(TEMP_DIR, filename)
            
            ensure_directory(save_path)
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            print(f"[ä¸‹è½½æˆåŠŸ] {url} â†’ {save_path}")
            downloaded_files.append(save_path)
        except Exception as e:
            print(f"[ä¸‹è½½å¤±è´¥] {url} - {str(e)}")
            continue
    return downloaded_files

def merge_hosts(files):
    """åˆå¹¶å»é‡é€»è¾‘"""
    entry_pattern = re.compile(r'^\s*(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s+(\S+)')
    domain_map = {}
    
    for file_path in files:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                match = entry_pattern.match(line)
                if match:
                    ip, domain = match.groups()
                    domain_map[domain.lower()] = ip  # æœ€åå‡ºç°çš„IPä¼šè¦†ç›–ä¹‹å‰çš„
    return domain_map

def main():
    print("\n=== å¼€å§‹åŒæ­¥æµç¨‹ ===")
    
    # é˜¶æ®µ1: ä¸‹è½½æ–‡ä»¶
    print("\n[é˜¶æ®µ1] ä¸‹è½½æºæ–‡ä»¶")
    hosts_files = download_hosts()
    if not hosts_files:
        print("é”™è¯¯: æœªä¸‹è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡ä»¶")
        return
    
    # æ·»åŠ æœ¬åœ°hosts.txt
    local_hosts_path = os.path.join(TEMP_DIR, "hosts.txt")
    if os.path.isfile(local_hosts_path):
        hosts_files.append(local_hosts_path)
        print(f"[æ·»åŠ æœ¬åœ°æ–‡ä»¶] {local_hosts_path}")
    else:
        print(f"[è­¦å‘Š] æœ¬åœ°æ–‡ä»¶ {local_hosts_path} ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡")
    
    # é˜¶æ®µ2: åˆå¹¶å¤„ç†
    print("\n[é˜¶æ®µ2] åˆå¹¶å¤„ç†")
    merged_data = merge_hosts(hosts_files)
    
    # é˜¶æ®µ3: ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶
    output_path = os.path.join(RULE_DIR, "merged_hosts.txt")
    ensure_directory(output_path)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# è§„åˆ™æ€»æ•°: {len(merged_data)}\n")  # ğŸ‘ˆ è§„åˆ™æ€»æ•°æ³¨é‡Šè¡Œ
        f.write("# é¡¹ç›®åœ°å€: https://github.com/045200/hyper_hosts\n\n")
        for domain, ip in sorted(merged_data.items()):
            f.write(f"{ip}\t{domain}\n")
    
    print(f"\n=== åŒæ­¥å®Œæˆ ===")
    print(f"ç”Ÿæˆæ–‡ä»¶: {output_path}")
    print(f"è§„åˆ™æ€»æ•°: {len(merged_data)}")

if __name__ == "__main__":
    main()
